{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIaPC67XzLtd",
        "outputId": "601e5b4f-678f-48cc-e407-bc24fd5383c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Any, List, Dict, Tuple, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –°–∫—Ä—ã–≤–∞–µ–º TensorFlow –ª–æ–≥–∏\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# –ò–º–ø–æ—Ä—Ç—ã ML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.sparse import csr_matrix\n",
        "import optuna\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏–º–ø–æ—Ä—Ç–æ–≤\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF –≤–º–µ—Å—Ç–æ BERT.\")\n",
        "\n",
        "try:\n",
        "    import implicit\n",
        "    IMPLICIT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IMPLICIT_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  implicit –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º ALS —Ñ–∏—á–∏.\")\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CATBOOST_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  catboost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ LightGBM.\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    TORCH_AVAILABLE = True\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# NLTK –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# –§–∏–∫—Å–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏\n",
        "def seed_everything(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    if TORCH_AVAILABLE:\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# === –ö–û–ù–°–¢–ê–ù–¢–´ ===\n",
        "class constants:\n",
        "    COL_USER_ID = \"user_id\"\n",
        "    COL_BOOK_ID = \"book_id\"\n",
        "    COL_TARGET = \"rating\"\n",
        "    COL_HAS_READ = \"has_read\"\n",
        "    COL_TIMESTAMP = \"timestamp\"\n",
        "    COL_PREDICTION = \"rating_predict\"\n",
        "\n",
        "    COL_GENDER = \"gender\"\n",
        "    COL_AGE = \"age\"\n",
        "    COL_AUTHOR_ID = \"author_id\"\n",
        "    COL_PUBLICATION_YEAR = \"publication_year\"\n",
        "    COL_LANGUAGE = \"language\"\n",
        "    COL_PUBLISHER = \"publisher\"\n",
        "    COL_AVG_RATING = \"avg_rating\"\n",
        "    COL_DESCRIPTION = \"description\"\n",
        "    COL_TITLE = \"title\"\n",
        "\n",
        "    # –ñ–∞–Ω—Ä—ã\n",
        "    COL_GENRE_ID = \"genre_id\"\n",
        "\n",
        "    F_USER_MEAN_RATING = \"user_mean_rating\"\n",
        "    F_USER_RATINGS_COUNT = \"user_ratings_count\"\n",
        "    F_BOOK_MEAN_RATING = \"book_mean_rating\"\n",
        "    F_BOOK_RATINGS_COUNT = \"book_ratings_count\"\n",
        "    F_AUTHOR_MEAN_RATING = \"author_mean_rating\"\n",
        "    F_BOOK_GENRES_COUNT = \"book_genres_count\"\n",
        "\n",
        "    MISSING_CAT_VALUE = \"unknown\"\n",
        "    MISSING_NUM_VALUE = -1\n",
        "    PREDICTION_MIN_VALUE = 0\n",
        "    PREDICTION_MAX_VALUE = 10\n",
        "\n",
        "class config:\n",
        "    RANDOM_STATE = 42\n",
        "    TARGET = constants.COL_TARGET\n",
        "\n",
        "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
        "    TEMPORAL_SPLIT_RATIO = 0.85\n",
        "\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
        "    EARLY_STOPPING_ROUNDS = 100\n",
        "\n",
        "    # –§–∞–π–ª—ã\n",
        "    MODEL_FILENAME = \"super_model\"\n",
        "    FEATURE_IMPORTANCE_PATH = \"feature_importance_final.csv\"\n",
        "\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
        "    USE_BERT_EMBEDDINGS = False  # –û—Ç–∫–ª—é—á–∞–µ–º BERT –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "    TFIDF_MAX_FEATURES = 300\n",
        "    TFIDF_MIN_DF = 3\n",
        "    TFIDF_MAX_DF = 0.9\n",
        "    TFIDF_NGRAM_RANGE = (1, 2)\n",
        "\n",
        "    # –ì—Ä–∞—Ñ–æ–≤—ã–µ —Ñ–∏—á–∏\n",
        "    ENABLE_GRAPH_FEATURES = False  # –û—Ç–∫–ª—é—á–∞–µ–º ALS –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "    # –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "    ENSEMBLE_MODELS = False  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ–∏—á–∏\n",
        "    ENABLE_TARGET_ENCODING = True\n",
        "    ENABLE_TEMPORAL_FEATURES = True\n",
        "    ENABLE_ADVANCED_INTERACTIONS = True\n",
        "    ENABLE_CLUSTER_FEATURES = False  # –û—Ç–∫–ª—é—á–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏\n",
        "    CAT_FEATURES = [\n",
        "        constants.COL_USER_ID,\n",
        "        constants.COL_BOOK_ID,\n",
        "        constants.COL_GENDER,\n",
        "        constants.COL_AUTHOR_ID,\n",
        "        constants.COL_LANGUAGE,\n",
        "        constants.COL_PUBLISHER,\n",
        "    ]\n",
        "\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM\n",
        "    LGB_PARAMS = {\n",
        "        \"objective\": \"rmse\",\n",
        "        \"metric\": \"rmse\",\n",
        "        \"n_estimators\": 5000,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"feature_fraction\": 0.8,\n",
        "        \"bagging_fraction\": 0.9,\n",
        "        \"bagging_freq\": 1,\n",
        "        \"lambda_l1\": 0.5,\n",
        "        \"lambda_l2\": 1.0,\n",
        "        \"num_leaves\": 63,\n",
        "        \"min_child_samples\": 30,\n",
        "        \"verbose\": -1,\n",
        "        \"n_jobs\": -1,\n",
        "        \"seed\": RANDOM_STATE,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"device\": \"cpu\",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "    }\n",
        "\n",
        "    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
        "    MEMORY_LIMIT_GB = 10\n",
        "\n",
        "# === –£–¢–ò–õ–ò–¢–´ ===\n",
        "def find_file(filename):\n",
        "    \"\"\"–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø–æ–¥–ø–∞–ø–∫–∞—Ö\"\"\"\n",
        "    for root, dirs, files in os.walk(\".\"):\n",
        "        if filename in files:\n",
        "            return os.path.join(root, filename)\n",
        "    raise FileNotFoundError(f\"–ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ {filename}!\")\n",
        "\n",
        "def safe_to_int(series, fill_value=0):\n",
        "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π NaN\"\"\"\n",
        "    return pd.to_numeric(series, errors='coerce').fillna(fill_value).astype(np.int8)\n",
        "\n",
        "def reduce_mem_usage(df: pd.DataFrame, aggressive=False) -> pd.DataFrame:\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤\"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"üìä –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–º—è—Ç–∏: {start_mem:.2f} MB\")\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã\n",
        "        if col_type == 'object' or str(col_type).startswith(('datetime', 'period', 'category')):\n",
        "            continue\n",
        "\n",
        "        if col_type.name != \"category\" and \"datetime\" not in str(col_type):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "            else:\n",
        "                if aggressive and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].round(5 if aggressive else 3).astype(np.float32)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {end_mem:.2f} MB (—Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ {100*(start_mem-end_mem)/start_mem:.1f}%)\")\n",
        "    return df\n",
        "\n",
        "def temporal_split_by_date(df: pd.DataFrame, split_ratio: float = 0.8) -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫\"\"\"\n",
        "    try:\n",
        "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π\n",
        "        if constants.COL_TIMESTAMP not in df.columns:\n",
        "            print(\"‚ö†Ô∏è  –ù–µ—Ç timestamp - —Å–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π\")\n",
        "            df[constants.COL_TIMESTAMP] = pd.date_range(\n",
        "                start='2020-01-01',\n",
        "                periods=len(df),\n",
        "                freq='D'\n",
        "            )\n",
        "        else:\n",
        "            if not pd.api.types.is_datetime64_any_dtype(df[constants.COL_TIMESTAMP]):\n",
        "                df[constants.COL_TIMESTAMP] = pd.to_datetime(\n",
        "                    df[constants.COL_TIMESTAMP],\n",
        "                    errors='coerce',\n",
        "                    infer_datetime_format=True,\n",
        "                    dayfirst=True\n",
        "                )\n",
        "\n",
        "            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "            if df[constants.COL_TIMESTAMP].isnull().any():\n",
        "                median_time = df[constants.COL_TIMESTAMP].median()\n",
        "                null_count = df[constants.COL_TIMESTAMP].isnull().sum()\n",
        "                print(f\"‚ö†Ô∏è  –ó–∞–ø–æ–ª–Ω—è–µ–º {null_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ timestamp –º–µ–¥–∏–∞–Ω–æ–π: {median_time}\")\n",
        "                df[constants.COL_TIMESTAMP] = df[constants.COL_TIMESTAMP].fillna(median_time)\n",
        "\n",
        "        df_sorted = df.sort_values(by=constants.COL_TIMESTAMP)\n",
        "        split_idx = int(len(df_sorted) * split_ratio)\n",
        "\n",
        "        train_mask = pd.Series(False, index=df.index)\n",
        "        val_mask = pd.Series(False, index=df.index)\n",
        "\n",
        "        train_mask.loc[df_sorted.index[:split_idx]] = True\n",
        "        val_mask.loc[df_sorted.index[split_idx:]] = True\n",
        "\n",
        "        return train_mask, val_mask\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {str(e)}\")\n",
        "        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç - —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
        "        print(\"‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç\")\n",
        "        train_mask = np.random.rand(len(df)) < split_ratio\n",
        "        return pd.Series(train_mask, index=df.index), pd.Series(~train_mask, index=df.index)\n",
        "\n",
        "def clean_text(text, lemmatize=False):\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
        "    if not isinstance(text, str) or pd.isna(text) or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # –£–¥–∞–ª—è–µ–º HTML\n",
        "    text = re.sub(r'http\\S+', '', text)  # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
        "    text = re.sub(r'\\d+', ' ', text)  # –£–¥–∞–ª—è–µ–º —Ü–∏—Ñ—Ä—ã\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
        "\n",
        "    return text\n",
        "\n",
        "# === –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===\n",
        "def load_and_merge_data():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üöÄ –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤\n",
        "    zip_files = glob.glob('*.zip')\n",
        "    if zip_files:\n",
        "        print(f\"üì¶ –ù–∞–π–¥–µ–Ω–æ zip –∞—Ä—Ö–∏–≤–æ–≤: {len(zip_files)}\")\n",
        "        for zip_file in zip_files:\n",
        "            print(f\"   –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º: {zip_file}\")\n",
        "            try:\n",
        "                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                    zip_ref.extractall('.')\n",
        "                print(f\"   ‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω: {zip_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ {zip_file}: {str(e)}\")\n",
        "\n",
        "    # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤\n",
        "    required_files = {\n",
        "        'train': ['train.csv', 'train_data.csv'],\n",
        "        'test': ['test.csv', 'test_data.csv'],\n",
        "        'books': ['books.csv', 'books_data.csv'],\n",
        "        'users': ['users.csv', 'users_data.csv'],\n",
        "        'descriptions': ['book_descriptions.csv', 'descriptions.csv'],\n",
        "        'genres': ['book_genres.csv', 'genres.csv']\n",
        "    }\n",
        "\n",
        "    file_paths = {}\n",
        "    optional_files = ['genres']\n",
        "\n",
        "    for key, filenames in required_files.items():\n",
        "        found = False\n",
        "        for filename in filenames:\n",
        "            try:\n",
        "                file_paths[key] = find_file(filename)\n",
        "                print(f\"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¥–ª—è {key}: {filename}\")\n",
        "                found = True\n",
        "                break\n",
        "            except FileNotFoundError:\n",
        "                continue\n",
        "\n",
        "        if not found:\n",
        "            if key in optional_files:\n",
        "                print(f\"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è {key}. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ.\")\n",
        "                file_paths[key] = None\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {key}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.\")\n",
        "\n",
        "    # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è\n",
        "    def detect_delimiter(file_path):\n",
        "        \"\"\"–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è CSV\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                first_line = f.readline()\n",
        "\n",
        "            if ';' in first_line and ',' in first_line:\n",
        "                if first_line.count(';') > first_line.count(','):\n",
        "                    return ';'\n",
        "            elif ';' in first_line:\n",
        "                return ';'\n",
        "            return ','\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è: {str(e)}. –ò—Å–ø–æ–ª—å–∑—É–µ–º ','\")\n",
        "            return ','\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "    def load_csv_safely(file_path, desc=\"\"):\n",
        "        \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CSV —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "        if not file_path:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        sep = detect_delimiter(file_path)\n",
        "        print(f\"   –ó–∞–≥—Ä—É–∂–∞–µ–º {Path(file_path).name} —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º '{sep}'\")\n",
        "\n",
        "        try:\n",
        "            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É\n",
        "            df = pd.read_csv(file_path, sep=sep, low_memory=False)\n",
        "            print(f\"   ‚úÖ {desc} shape: {df.shape}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {Path(file_path).name}: {str(e)}\")\n",
        "            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –æ—à–∏–±–æ–∫\n",
        "            try:\n",
        "                df = pd.read_csv(file_path, sep=sep, on_bad_lines='skip', low_memory=False)\n",
        "                print(f\"   ‚ö†Ô∏è  –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –æ—à–∏–±–æ—á–Ω—ã—Ö —Å—Ç—Ä–æ–∫. Shape: {df.shape}\")\n",
        "                return df\n",
        "            except Exception as e2:\n",
        "                print(f\"   ‚ùå –ü–æ–ª–Ω—ã–π –ø—Ä–æ–≤–∞–ª –∑–∞–≥—Ä—É–∑–∫–∏: {str(e2)}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    train_df = load_csv_safely(file_paths['train'], \"Train\")\n",
        "    test_df = load_csv_safely(file_paths['test'], \"Test\")\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ has_read\n",
        "    if 'has_read' in train_df.columns:\n",
        "        original_len = len(train_df)\n",
        "        train_df = train_df[train_df['has_read'] == 1].copy()\n",
        "        print(f\"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ has_read=1: {original_len - len(train_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    books_df = load_csv_safely(file_paths['books'], \"Books\")\n",
        "    users_df = load_csv_safely(file_paths['users'], \"Users\")\n",
        "    desc_df = load_csv_safely(file_paths['descriptions'], \"Descriptions\")\n",
        "    genres_df = load_csv_safely(file_paths['genres'], \"Genres\") if file_paths['genres'] else None\n",
        "\n",
        "    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
        "    print(\"\\nüîó –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫\n",
        "    train_df['_source'] = 'train'\n",
        "    test_df['_source'] = 'test'\n",
        "\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ test\n",
        "    combined = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "    # –ú–µ—Ä–¥–∂–∏–º —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏\n",
        "    print(\"   ‚Üí –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏\")\n",
        "    if 'user_id' in users_df.columns and 'user_id' in combined.columns:\n",
        "        combined = combined.merge(users_df, on='user_id', how='left')\n",
        "        print(f\"      –ü–æ—Å–ª–µ –º–µ—Ä–∂–∞: {combined.shape}\")\n",
        "\n",
        "    # –ú–µ—Ä–¥–∂–∏–º —Å –∫–Ω–∏–≥–∞–º–∏\n",
        "    print(\"   ‚Üí –ö–Ω–∏–≥–∏\")\n",
        "    if 'book_id' in books_df.columns and 'book_id' in combined.columns:\n",
        "        books_df = books_df.drop_duplicates(subset=['book_id'])\n",
        "        combined = combined.merge(books_df, on='book_id', how='left')\n",
        "        print(f\"      –ü–æ—Å–ª–µ –º–µ—Ä–∂–∞: {combined.shape}\")\n",
        "\n",
        "    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏\n",
        "    combined = reduce_mem_usage(combined, aggressive=True)\n",
        "\n",
        "    print(f\"‚úÖ –û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {time.time() - start_time:.1f} —Å–µ–∫\")\n",
        "    print(f\"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {combined.shape}\")\n",
        "\n",
        "    return combined, desc_df, genres_df, train_df, test_df\n",
        "\n",
        "# === –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–†–û–î–í–ò–ù–£–¢–´–• –§–ò–ß–ï–ô ===\n",
        "def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    if not config.ENABLE_TEMPORAL_FEATURES or constants.COL_TIMESTAMP not in df.columns:\n",
        "        return df\n",
        "\n",
        "    print(\"‚è±Ô∏è  –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...\")\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime —Å –∑–∞—â–∏—Ç–æ–π\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[constants.COL_TIMESTAMP]):\n",
        "        try:\n",
        "            df[constants.COL_TIMESTAMP] = pd.to_datetime(\n",
        "                df[constants.COL_TIMESTAMP],\n",
        "                errors='coerce',\n",
        "                infer_datetime_format=True,\n",
        "                dayfirst=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è timestamp: {str(e)}\")\n",
        "            # –°–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π timestamp\n",
        "            df[constants.COL_TIMESTAMP] = pd.date_range(\n",
        "                start='2020-01-01',\n",
        "                periods=len(df),\n",
        "                freq='D'\n",
        "            )\n",
        "\n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    if df[constants.COL_TIMESTAMP].isnull().any():\n",
        "        median_time = df[constants.COL_TIMESTAMP].median()\n",
        "        null_count = df[constants.COL_TIMESTAMP].isnull().sum()\n",
        "        print(f\"‚ö†Ô∏è  –ó–∞–ø–æ–ª–Ω—è–µ–º {null_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ timestamp: {median_time}\")\n",
        "        df[constants.COL_TIMESTAMP] = df[constants.COL_TIMESTAMP].fillna(median_time)\n",
        "\n",
        "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
        "    try:\n",
        "        df['hour'] = df[constants.COL_TIMESTAMP].dt.hour\n",
        "        df['day'] = df[constants.COL_TIMESTAMP].dt.day\n",
        "        df['month'] = df[constants.COL_TIMESTAMP].dt.month\n",
        "        df['year'] = df[constants.COL_TIMESTAMP].dt.year\n",
        "        df['dayofweek'] = df[constants.COL_TIMESTAMP].dt.dayofweek\n",
        "        df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(np.int8)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {str(e)}\")\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        df['hour'] = 12\n",
        "        df['dayofweek'] = 3\n",
        "        df['month'] = 6\n",
        "        df['is_weekend'] = 0\n",
        "\n",
        "    # –ü–µ—Ä–∏–æ–¥—ã –¥–Ω—è —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π\n",
        "    try:\n",
        "        time_period = pd.cut(\n",
        "            df['hour'].fillna(12),\n",
        "            bins=[-1, 6, 12, 18, 24],\n",
        "            labels=[0, 1, 2, 3],\n",
        "            include_lowest=True\n",
        "        )\n",
        "        df['time_period'] = safe_to_int(time_period, fill_value=1)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ time_period: {str(e)}\")\n",
        "        df['time_period'] = 1\n",
        "\n",
        "    # –°–µ–∑–æ–Ω—ã\n",
        "    try:\n",
        "        df['month_filled'] = df['month'].fillna(6)\n",
        "        df['season'] = ((df['month_filled'] - 1) // 3 + 1).astype(np.int8)\n",
        "        df.drop(columns=['month_filled'], inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ seasons: {str(e)}\")\n",
        "        df['season'] = 2\n",
        "\n",
        "    print(f\"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∏—á: {len([c for c in df.columns if 'time' in c or 'hour' in c or 'day' in c or 'month' in c or 'year' in c])}\")\n",
        "    return df\n",
        "\n",
        "def add_target_encoding(df: pd.DataFrame, train_split: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ target encoding —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —É—Ç–µ—á–µ–∫ –∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤\"\"\"\n",
        "    if not config.ENABLE_TARGET_ENCODING or train_split is None or constants.COL_TARGET not in train_split.columns:\n",
        "        return df\n",
        "\n",
        "    print(\"üéØ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º target encoding...\")\n",
        "\n",
        "    # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ\n",
        "    global_mean = train_split[constants.COL_TARGET].mean()\n",
        "\n",
        "    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "    encode_cols = []\n",
        "    for col in ['author_id', 'publisher', 'language']:\n",
        "        if col in df.columns and col in train_split.columns:\n",
        "            if train_split[col].nunique() > 1:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –≤–∞—Ä–∏–∞—Ü–∏–∏\n",
        "                encode_cols.append(col)\n",
        "\n",
        "    for col in encode_cols:\n",
        "        print(f\"   ‚Üí {col}\")\n",
        "\n",
        "        try:\n",
        "            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞ train\n",
        "            agg = train_split.groupby(col, dropna=False)[constants.COL_TARGET].agg(\n",
        "                ['mean', 'count']\n",
        "            ).reset_index()\n",
        "\n",
        "            # Bayesian smoothing\n",
        "            agg['smoothed_mean'] = (\n",
        "                agg['mean'] * agg['count'] + global_mean * 10\n",
        "            ) / (agg['count'] + 10)\n",
        "\n",
        "            # –ú–µ—Ä–¥–∂–∏–º\n",
        "            rename_cols = {'smoothed_mean': f'{col}_target_mean'}\n",
        "            df = df.merge(\n",
        "                agg[[col] + list(rename_cols.keys())].rename(columns=rename_cols),\n",
        "                on=col,\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "            for new_col in rename_cols.values():\n",
        "                if new_col in df.columns:\n",
        "                    fill_value = global_mean\n",
        "                    null_count_before = df[new_col].isnull().sum()\n",
        "                    df[new_col] = df[new_col].fillna(fill_value)\n",
        "                    null_count_after = df[new_col].isnull().sum()\n",
        "\n",
        "                    if null_count_after > 0:\n",
        "                        print(f\"   ‚ö†Ô∏è  –û—Å—Ç–∞–ª–æ—Å—å {null_count_after} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ {new_col} –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {col}: {str(e)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_graph_features(df, train_df):\n",
        "    \"\"\"–ì—Ä–∞—Ñ–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ ALS —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    return df  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "def add_bert_embeddings(df, desc_df, train_books):\n",
        "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Sentence-BERT —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    return add_text_features(df, desc_df, train_books)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF –≤–º–µ—Å—Ç–æ BERT\n",
        "\n",
        "def add_text_features(df: pd.DataFrame, desc_df: pd.DataFrame, train_books: list):\n",
        "    \"\"\"–î–æ–±–∞–≤–ª—è–µ–º TF-IDF —Ñ–∏—á–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    print(\"üî§ –î–æ–±–∞–≤–ª—è–µ–º TF-IDF —Ñ–∏—á–∏...\")\n",
        "\n",
        "    if desc_df.empty or len(train_books) == 0:\n",
        "        print(\"   ‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è TF-IDF\")\n",
        "        return df\n",
        "\n",
        "    try:\n",
        "        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π\n",
        "        desc_df = desc_df.copy()\n",
        "        if 'description' in desc_df.columns:\n",
        "            desc_df['description_clean'] = desc_df['description'].apply(clean_text)\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'description' –≤ –¥–∞–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π\")\n",
        "            desc_df['description_clean'] = \"\"\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –æ–ø–∏—Å–∞–Ω–∏–π\n",
        "        desc_map = dict(zip(desc_df['book_id'], desc_df['description_clean'].fillna(\"no description\")))\n",
        "\n",
        "        # –£—á–∏–º TF-IDF —Ç–æ–ª—å–∫–æ –Ω–∞ train –∫–Ω–∏–≥–∞—Ö\n",
        "        train_desc_df = desc_df[desc_df['book_id'].isin(train_books)].copy()\n",
        "\n",
        "        if len(train_desc_df) < 10:\n",
        "            print(\"   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TF-IDF\")\n",
        "            return df\n",
        "\n",
        "        # –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä\n",
        "        tfidf = TfidfVectorizer(\n",
        "            max_features=config.TFIDF_MAX_FEATURES,\n",
        "            min_df=config.TFIDF_MIN_DF,\n",
        "            max_df=config.TFIDF_MAX_DF,\n",
        "            ngram_range=config.TFIDF_NGRAM_RANGE,\n",
        "            stop_words=None,\n",
        "            token_pattern=r'(?u)\\b\\w+\\b',\n",
        "            lowercase=False\n",
        "        )\n",
        "\n",
        "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
        "        try:\n",
        "            tfidf.fit(train_desc_df['description_clean'])\n",
        "            print(f\"   –û–±—É—á–µ–Ω TF-IDF —Å {len(tfidf.get_feature_names_out())} —Ñ–∏—á–∞–º–∏\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ TF-IDF: {str(e)}\")\n",
        "            return df\n",
        "\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –æ–ø–∏—Å–∞–Ω–∏—è–º –≤—Å–µ—Ö –∫–Ω–∏–≥\n",
        "        all_books = df['book_id'].unique()\n",
        "        all_descriptions = [desc_map.get(book_id, \"no description\") for book_id in all_books]\n",
        "\n",
        "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "        try:\n",
        "            tfidf_matrix = tfidf.transform(all_descriptions)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ TF-IDF: {str(e)}\")\n",
        "            return df\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º DataFrame\n",
        "        tfidf_df = pd.DataFrame(\n",
        "            tfidf_matrix.toarray(),\n",
        "            index=all_books,\n",
        "            columns=[f'tfidf_{i}' for i in range(min(tfidf_matrix.shape[1], config.TFIDF_MAX_FEATURES))]\n",
        "        ).reset_index().rename(columns={'index': 'book_id'})\n",
        "\n",
        "        # –ú–µ—Ä–¥–∂–∏–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–º\n",
        "        df = df.merge(tfidf_df, on='book_id', how='left')\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "        tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]\n",
        "        if tfidf_cols:\n",
        "            df['tfidf_sum'] = df[tfidf_cols].sum(axis=1)\n",
        "            df['tfidf_mean'] = df[tfidf_cols].mean(axis=1)\n",
        "            df['tfidf_max'] = df[tfidf_cols].max(axis=1)\n",
        "\n",
        "        print(f\"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(tfidf_cols)} TF-IDF —Ñ–∏—á + 3 —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ TF-IDF: {str(e)}\")\n",
        "        return df\n",
        "\n",
        "def add_cluster_features(df, n_clusters=10):\n",
        "    \"\"\"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    return df  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "def add_aggregate_features(df: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã–µ —Ñ–∏—á–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    print(\"üìä –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã–µ —Ñ–∏—á–∏...\")\n",
        "\n",
        "    if train_df is None or train_df.empty:\n",
        "        print(\"   ‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã—Ö —Ñ–∏—á–µ–π\")\n",
        "        return df\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ train –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫\n",
        "    train_data = train_df[train_df['_source'] == 'train'].copy()\n",
        "\n",
        "    if train_data.empty:\n",
        "        print(\"   ‚ö†Ô∏è  –ù–µ—Ç train –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏\")\n",
        "        return df\n",
        "\n",
        "    # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º\n",
        "    print(\"   ‚Üí –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∞–≥—Ä–µ–≥–∞—Ç—ã\")\n",
        "    try:\n",
        "        user_agg = train_data.groupby('user_id', dropna=False).agg(\n",
        "            user_mean_rating=(constants.COL_TARGET, 'mean'),\n",
        "            user_ratings_count=(constants.COL_TARGET, 'count'),\n",
        "            user_rating_std=(constants.COL_TARGET, 'std'),\n",
        "        ).reset_index()\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ std\n",
        "        user_agg['user_rating_std'] = user_agg['user_rating_std'].fillna(0)\n",
        "        df = df.merge(user_agg, on='user_id', how='left')\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ–≤: {str(e)}\")\n",
        "\n",
        "    # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –∫–Ω–∏–≥–∞–º\n",
        "    print(\"   ‚Üí –ö–Ω–∏–∂–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç—ã\")\n",
        "    try:\n",
        "        book_agg = train_data.groupby('book_id', dropna=False).agg(\n",
        "            book_mean_rating=(constants.COL_TARGET, 'mean'),\n",
        "            book_ratings_count=(constants.COL_TARGET, 'count'),\n",
        "            book_rating_std=(constants.COL_TARGET, 'std'),\n",
        "        ).reset_index()\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ std\n",
        "        book_agg['book_rating_std'] = book_agg['book_rating_std'].fillna(0)\n",
        "        df = df.merge(book_agg, on='book_id', how='left')\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–Ω–∏–∂–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ–≤: {str(e)}\")\n",
        "\n",
        "    # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –∞–≤—Ç–æ—Ä–∞–º\n",
        "    if 'author_id' in train_data.columns and train_data['author_id'].notnull().any():\n",
        "        print(\"   ‚Üí –ê–≤—Ç–æ—Ä—Å–∫–∏–µ –∞–≥—Ä–µ–≥–∞—Ç—ã\")\n",
        "        try:\n",
        "            author_agg = train_data.groupby('author_id', dropna=False).agg(\n",
        "                author_mean_rating=(constants.COL_TARGET, 'mean'),\n",
        "                author_ratings_count=(constants.COL_TARGET, 'count'),\n",
        "                author_rating_std=(constants.COL_TARGET, 'std')\n",
        "            ).reset_index()\n",
        "\n",
        "            author_agg['author_rating_std'] = author_agg['author_rating_std'].fillna(0)\n",
        "            df = df.merge(author_agg, on='author_id', how='left')\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ–≤: {str(e)}\")\n",
        "\n",
        "    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
        "    if config.ENABLE_ADVANCED_INTERACTIONS:\n",
        "        print(\"   ‚Üí –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\")\n",
        "\n",
        "        try:\n",
        "            # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Å—Ä–µ–¥–Ω–µ–π –∫–Ω–∏–≥–æ–π –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º\n",
        "            if 'book_mean_rating' in df.columns and 'user_mean_rating' in df.columns:\n",
        "                df['rating_diff_user_book'] = df['book_mean_rating'] - df['user_mean_rating']\n",
        "\n",
        "            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞\n",
        "            if 'rating_diff_user_book' in df.columns and 'user_rating_std' in df.columns and 'book_rating_std' in df.columns:\n",
        "                df['rating_diff_normalized'] = (\n",
        "                    df['rating_diff_user_book'] /\n",
        "                    (df['user_rating_std'] + df['book_rating_std'] + 1e-6)\n",
        "                ).astype(np.float32)\n",
        "\n",
        "            # –û—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ü–µ–Ω–æ–∫\n",
        "            if 'user_ratings_count' in df.columns and 'book_ratings_count' in df.columns:\n",
        "                df['rating_count_ratio'] = (\n",
        "                    np.log1p(df['user_ratings_count']) /\n",
        "                    (np.log1p(df['book_ratings_count']) + 1)\n",
        "                ).astype(np.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {str(e)}\")\n",
        "\n",
        "    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
        "    for col in ['user_ratings_count', 'book_ratings_count']:\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                df[f'log_{col}'] = np.log1p(df[col]).astype(np.float32)\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ {col}: {str(e)}\")\n",
        "\n",
        "    print(\"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã—Ö —Ñ–∏—á–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
        "    return df\n",
        "\n",
        "def prepare_data_for_lgbm(df):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM - —É–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ —Ñ–∏—á–∏\"\"\"\n",
        "    print(\"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM...\")\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    str_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    if str_cols:\n",
        "        print(f\"   –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {str_cols}\")\n",
        "        df = df.drop(columns=str_cols)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    datetime_cols = df.select_dtypes(include=['datetime', 'datetimetz']).columns.tolist()\n",
        "    if datetime_cols:\n",
        "        print(f\"   –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {datetime_cols}\")\n",
        "        df = df.drop(columns=datetime_cols)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    cat_cols_to_drop = []\n",
        "    for col in df.select_dtypes(include=['category']).columns:\n",
        "        if df[col].nunique() > 1000:  # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "            cat_cols_to_drop.append(col)\n",
        "\n",
        "    if cat_cols_to_drop:\n",
        "        print(f\"   –£–¥–∞–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {cat_cols_to_drop}\")\n",
        "        df = df.drop(columns=cat_cols_to_drop)\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç\n",
        "    for col in df.select_dtypes(include=['category']).columns:\n",
        "        df[col] = df[col].cat.codes\n",
        "\n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            if df[col].dtype in ['float64', 'float32']:\n",
        "                fill_value = df[col].median()\n",
        "            else:\n",
        "                fill_value = -1\n",
        "            df[col] = df[col].fillna(fill_value)\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Å—Ç–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã\n",
        "    non_numeric = df.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    if non_numeric:\n",
        "        print(f\"   ‚ö†Ô∏è  –í—Å–µ –µ—â–µ –µ—Å—Ç—å –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {non_numeric}\")\n",
        "        df = df.drop(columns=non_numeric)\n",
        "\n",
        "    print(f\"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(df.columns)} —á–∏—Å–ª–æ–≤—ã—Ö —Ñ–∏—á –¥–ª—è LightGBM\")\n",
        "    return df\n",
        "\n",
        "def handle_missing_values(df: pd.DataFrame, train_stats: dict = None):\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π\"\"\"\n",
        "    print(\"üß© –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤...\")\n",
        "\n",
        "    # 1. –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö\n",
        "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    for col in num_cols:\n",
        "        if col in df.columns and df[col].isnull().any():\n",
        "            null_count = df[col].isnull().sum()\n",
        "\n",
        "            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–ª–æ–Ω–∫–∏\n",
        "            fill_value = None\n",
        "\n",
        "            if train_stats and col in train_stats:\n",
        "                fill_value = train_stats[col]\n",
        "            elif col in ['user_mean_rating', 'book_mean_rating', 'author_mean_rating']:\n",
        "                fill_value = df[constants.COL_TARGET].mean() if constants.COL_TARGET in df.columns else 5.0\n",
        "            elif 'count' in col.lower() or 'rating_count' in col.lower():\n",
        "                fill_value = 0\n",
        "            elif 'std' in col.lower():\n",
        "                fill_value = 0\n",
        "            elif 'diff' in col.lower() or 'ratio' in col.lower():\n",
        "                fill_value = 0\n",
        "            elif col in ['age', 'publication_year']:\n",
        "                fill_value = df[col].median() if df[col].notnull().any() else 35\n",
        "            else:\n",
        "                fill_value = df[col].median() if df[col].notnull().any() else 0\n",
        "\n",
        "            df[col] = df[col].fillna(fill_value)\n",
        "            new_null_count = df[col].isnull().sum()\n",
        "\n",
        "            if new_null_count > 0:\n",
        "                print(f\"   ‚ö†Ô∏è  –û—Å—Ç–∞–ª–æ—Å—å {new_null_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ {col} –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è\")\n",
        "                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
        "                df[col] = df[col].fillna(fill_value)\n",
        "\n",
        "    # 2. –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö\n",
        "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    for col in cat_cols:\n",
        "        if col in df.columns and df[col].isnull().any():\n",
        "            fill_value = constants.MISSING_CAT_VALUE\n",
        "            null_count = df[col].isnull().sum()\n",
        "            df[col] = df[col].fillna(fill_value)\n",
        "            print(f\"   ‚Üí {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ {null_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∑–Ω–∞—á–µ–Ω–∏–µ–º '{fill_value}'\")\n",
        "\n",
        "    # 3. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ\n",
        "    remaining_nulls = df.isnull().sum()\n",
        "    remaining_nulls = remaining_nulls[remaining_nulls > 0]\n",
        "\n",
        "    if len(remaining_nulls) > 0:\n",
        "        print(f\"   ‚ö†Ô∏è  –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {remaining_nulls.to_dict()}\")\n",
        "        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "        for col in remaining_nulls.index:\n",
        "            if df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']:\n",
        "                df[col] = df[col].fillna(0)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(constants.MISSING_CAT_VALUE)\n",
        "        print(\"   ‚úÖ –í—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–ø—É—Å–∫–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# === –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–ï ===\n",
        "def train_model(X_train, y_train, X_val, y_val, cat_features):\n",
        "    \"\"\"–û–±—É—á–µ–Ω–∏–µ LightGBM –º–æ–¥–µ–ª–∏ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "    print(\"ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LightGBM...\")\n",
        "\n",
        "    try:\n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è LightGBM\n",
        "        X_train_prepared = prepare_data_for_lgbm(X_train.copy())\n",
        "        X_val_prepared = prepare_data_for_lgbm(X_val.copy())\n",
        "\n",
        "        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ —Ñ–∏—á–∏\n",
        "        common_features = list(set(X_train_prepared.columns) & set(X_val_prepared.columns))\n",
        "        X_train_prepared = X_train_prepared[common_features]\n",
        "        X_val_prepared = X_val_prepared[common_features]\n",
        "\n",
        "        print(f\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(common_features)} —Ñ–∏—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\")\n",
        "\n",
        "        # LightGBM\n",
        "        train_data = lgb.Dataset(\n",
        "            X_train_prepared,\n",
        "            label=y_train,\n",
        "            free_raw_data=False\n",
        "        )\n",
        "\n",
        "        val_data = lgb.Dataset(\n",
        "            X_val_prepared,\n",
        "            label=y_val,\n",
        "            reference=train_data,\n",
        "            free_raw_data=False\n",
        "        )\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ\n",
        "        model = lgb.train(\n",
        "            config.LGB_PARAMS,\n",
        "            train_data,\n",
        "            valid_sets=[train_data, val_data],\n",
        "            valid_names=['train', 'val'],\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=config.EARLY_STOPPING_ROUNDS),\n",
        "                lgb.log_evaluation(period=50),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return model, common_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LightGBM: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def adaptive_calibration(preds, y_true=None, strategy='median'):\n",
        "    \"\"\"–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\"\"\"\n",
        "    print(\"üéØ –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∫–∞–ª–∏–±—Ä–æ–≤–∫—É...\")\n",
        "\n",
        "    preds = np.array(preds).copy()\n",
        "    original_median = np.median(preds)\n",
        "\n",
        "    if strategy == 'median' and y_true is not None:\n",
        "        target_median = np.median(y_true)\n",
        "        shift = target_median - original_median\n",
        "        preds += shift * 0.8  # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ\n",
        "        print(f\"   –°–¥–≤–∏–≥ –º–µ–¥–∏–∞–Ω—ã: {shift:.4f} (—Ü–µ–ª—å: {target_median:.2f})\")\n",
        "\n",
        "    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞\n",
        "    preds = np.clip(preds, constants.PREDICTION_MIN_VALUE + 0.5, constants.PREDICTION_MAX_VALUE - 0.5)\n",
        "\n",
        "    new_median = np.median(preds)\n",
        "    print(f\"   –ú–µ–¥–∏–∞–Ω–∞ –¥–æ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {original_median:.4f} ‚Üí –ø–æ—Å–ª–µ: {new_median:.4f}\")\n",
        "\n",
        "    return preds\n",
        "\n",
        "# === –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù ===\n",
        "def main_pipeline():\n",
        "    \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ñ–∏—á–∞–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üöÄ –ó–ê–ü–£–°–ö –°–£–ü–ï–†-–ü–ê–ô–ü–õ–ê–ô–ù–ê\")\n",
        "    print(f\"   –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        combined, desc_df, genres_df, original_train_df, test_df = load_and_merge_data()\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ñ–∏—á–µ–π\n",
        "        combined = add_temporal_features(combined)\n",
        "\n",
        "        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ train –∫–Ω–∏–≥\n",
        "        train_books = original_train_df['book_id'].unique() if not original_train_df.empty else []\n",
        "\n",
        "        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏\n",
        "        combined = add_text_features(combined, desc_df, train_books)\n",
        "\n",
        "        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train –∏ test\n",
        "        train_processed = combined[combined['_source'] == 'train'].copy()\n",
        "        test_processed = combined[combined['_source'] == 'test'].copy()\n",
        "\n",
        "        # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ø–ª–∏—Ç\n",
        "        print(\"\\n‚è±Ô∏è  –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ø–ª–∏—Ç...\")\n",
        "        train_mask, val_mask = temporal_split_by_date(train_processed, config.TEMPORAL_SPLIT_RATIO)\n",
        "\n",
        "        train_split = train_processed.loc[train_mask].copy()\n",
        "        val_split = train_processed.loc[val_mask].copy()\n",
        "\n",
        "        print(f\"   Train split: {len(train_split):,} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        print(f\"   Val split: {len(val_split):,} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ñ–∏—á–µ–π\n",
        "        print(\"\\n‚ú® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ñ–∏—á–µ–π –¥–ª—è train...\")\n",
        "        train_split = add_aggregate_features(train_split, train_split)\n",
        "        train_split = add_target_encoding(train_split, train_split)\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "        train_stats = {}\n",
        "        for col in train_split.select_dtypes(include=np.number).columns:\n",
        "            if col != constants.COL_TARGET:\n",
        "                train_stats[col] = train_split[col].median()\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "        train_split = handle_missing_values(train_split, train_stats)\n",
        "        val_split = add_aggregate_features(val_split, train_split)\n",
        "        val_split = add_target_encoding(val_split, train_split)\n",
        "        val_split = handle_missing_values(val_split, train_stats)\n",
        "\n",
        "        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏\n",
        "        print(\"\\nüíæ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...\")\n",
        "        train_split = reduce_mem_usage(train_split, aggressive=True)\n",
        "        val_split = reduce_mem_usage(val_split, aggressive=True)\n",
        "        train_processed = reduce_mem_usage(train_processed, aggressive=True)\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è target –∫–æ–ª–æ–Ω–∫–∏\n",
        "        if constants.COL_TARGET not in train_split.columns:\n",
        "            if 'rating' in train_split.columns:\n",
        "                train_split[constants.COL_TARGET] = train_split['rating']\n",
        "                val_split[constants.COL_TARGET] = val_split['rating']\n",
        "                print(\"‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É 'rating' –∫–∞–∫ target\")\n",
        "            else:\n",
        "                raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ target: {constants.COL_TARGET}\")\n",
        "\n",
        "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        exclude_cols = [\n",
        "            '_source', 'timestamp', 'has_read', 'title', 'author_name', 'rating',\n",
        "            'description', 'description_clean', 'book_description'\n",
        "        ]\n",
        "\n",
        "        features = [col for col in train_split.columns if col not in exclude_cols]\n",
        "\n",
        "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        X_train = train_split[features]\n",
        "        y_train = train_split[constants.COL_TARGET]\n",
        "        X_val = val_split[features]\n",
        "        y_val = val_split[constants.COL_TARGET]\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"ü§ñ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        model, used_features = train_model(X_train, y_train, X_val, y_val, [])\n",
        "\n",
        "        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "        X_val_prepared = prepare_data_for_lgbm(X_val.copy())\n",
        "        X_val_prepared = X_val_prepared[used_features]\n",
        "        val_preds = model.predict(X_val_prepared)\n",
        "\n",
        "        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞\n",
        "        val_preds = adaptive_calibration(val_preds, y_val, strategy='median')\n",
        "\n",
        "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "        mae = mean_absolute_error(y_val, val_preds)\n",
        "        score = 1 - ((rmse/10) + (mae/10)) / 2\n",
        "\n",
        "        print(f\"\\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\")\n",
        "        print(f\"   RMSE: {rmse:.5f}\")\n",
        "        print(f\"   MAE: {mae:.5f}\")\n",
        "        print(f\"   SCORE: {score:.5f}\")\n",
        "        print(f\"   –ú–µ–¥–∏–∞–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {np.median(val_preds):.2f}\")\n",
        "        print(f\"   –°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {val_preds.mean():.2f}\")\n",
        "\n",
        "        # –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π\n",
        "        try:\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'feature': used_features,\n",
        "                'importance': model.feature_importance('gain')\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            feature_importance.to_csv(config.FEATURE_IMPORTANCE_PATH, index=False)\n",
        "            print(f\"‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.FEATURE_IMPORTANCE_PATH}\")\n",
        "            print(\"\\nüîë –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö —Ñ–∏—á–µ–π:\")\n",
        "            for _, row in feature_importance.head(10).iterrows():\n",
        "                print(f\"   {row['feature']}: {row['importance']:.0f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π: {str(e)}\")\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üîÆ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "        test_processed = add_aggregate_features(test_processed, train_processed[train_processed['_source'] == 'train'])\n",
        "        test_processed = add_target_encoding(test_processed, train_split)\n",
        "        test_processed = handle_missing_values(test_processed, train_stats)\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ–∏—á–µ–π\n",
        "        X_test = test_processed[features]\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "        print(\"   –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...\")\n",
        "        X_test_prepared = prepare_data_for_lgbm(X_test.copy())\n",
        "        X_test_prepared = X_test_prepared[used_features]\n",
        "        test_preds = model.predict(X_test_prepared)\n",
        "\n",
        "        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞\n",
        "        test_preds = adaptive_calibration(test_preds, y_train, strategy='median')\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "        print(f\"\\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
        "        print(f\"   –ú–µ–¥–∏–∞–Ω–∞: {np.median(test_preds):.4f}\")\n",
        "        print(f\"   –°—Ä–µ–¥–Ω–µ–µ: {test_preds.mean():.4f}\")\n",
        "        print(f\"   Min: {test_preds.min():.4f}\")\n",
        "        print(f\"   Max: {test_preds.max():.4f}\")\n",
        "        print(f\"   Std: {test_preds.std():.4f}\")\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "        submission = test_df[['user_id', 'book_id']].copy()\n",
        "        submission['rating_predict'] = test_preds\n",
        "\n",
        "        # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ\n",
        "        submission['rating_predict'] = submission['rating_predict'].round(2)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"submission_{timestamp}_{score:.5f}.csv\"\n",
        "        submission.to_csv(filename, index=False)\n",
        "        print(f\"\\n‚úÖ –°–∞–±–º–∏—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}\")\n",
        "        print(f\"   –†–∞–∑–º–µ—Ä: {submission.shape}\")\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "        try:\n",
        "            model.save_model(f\"{config.MODEL_FILENAME}_{timestamp}.txt\")\n",
        "            print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.MODEL_FILENAME}_{timestamp}.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}\")\n",
        "\n",
        "        # –û–±—â–µ–µ –≤—Ä–µ–º—è\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n{'=' * 80}\")\n",
        "        print(f\"üèÅ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù –ó–ê {total_time:.1f} –°–ï–ö–£–ù–î\")\n",
        "        print(f\"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä: {score:.5f}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return score, submission, model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–∞–±–º–∏—Ç\n",
        "        print(\"\\nüíæ –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–∞–±–º–∏—Ç...\")\n",
        "        try:\n",
        "            if 'test_df' in locals() and test_df is not None:\n",
        "                submission = test_df[['user_id', 'book_id']].copy()\n",
        "                submission['rating_predict'] = 5.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
        "\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                backup_filename = f\"submission_backup_{timestamp}.csv\"\n",
        "                submission.to_csv(backup_filename, index=False)\n",
        "                print(f\"‚úÖ –†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–∞–±–º–∏—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {backup_filename}\")\n",
        "                return 0.5, submission, None\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–∞–±–º–∏—Ç - –Ω–µ—Ç test_df\")\n",
        "                return 0.5, None, None\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ —Å–∞–±–º–∏—Ç–∞: {str(e2)}\")\n",
        "            return 0.5, None, None\n",
        "\n",
        "# === –ó–ê–ü–£–°–ö ===\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "l-_-g-HNbZqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d160c62f-47cb-4499-a713-af40c29af0d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttrain's rmse: 2.42504\tval's rmse: 2.888\n",
            "[100]\ttrain's rmse: 2.19031\tval's rmse: 2.87137\n",
            "[150]\ttrain's rmse: 2.0861\tval's rmse: 2.89194\n",
            "[200]\ttrain's rmse: 2.03675\tval's rmse: 2.91703\n",
            "Early stopping, best iteration is:\n",
            "[104]\ttrain's rmse: 2.17934\tval's rmse: 2.86975\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}